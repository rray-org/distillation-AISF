# The easy goal inference problem is still hard

Один из подходов к проблеме безопасности ИИ выглядит следующим образом:

- Наблюдение за тем, что говорит и делает пользователь системы.
- Вывод о предпочтениях пользователя.
- Попытки сделать мир лучше в соответствии с предпочтениями пользователя, возможно, работая вместе с ним и задавая уточняющие вопросы.

Преимущество этого подхода в том, что мы можем начать эмпирическую работу уже сегодня. Было бы прекрасно развивать это параллельно с другими достижениями ИИ и решать возникающие трудности по мере их появления

Такой подход дает нам конкретную модель каждой трудности, которую мы пытаемся решить, и достаточно четкий показатель того, способны ли мы управлять ИИ так же хорошо как его создавать. А благодаря тому, что сейчас он технически интересен и экономически значим, он может помочь реально интегрировать управление ИИ в практику ИИ. В целом автор находит, что это очень перспективный подход к проблеме безопасности ИИ

## Моделируя несовершенство

Однако автор считает, что этот подход основывается на оптимистичном предположении: что можно смоделировать человека как несовершенного рационального агента и извлечь реальные ценности, которые он так же несовершенно выводит с помощью оптимизаций.

Чтобы отделить эту проблему, мы можем рассмотреть значительно упрощённый способ вывода цели: в условиях ограченной информации найдите любое разумное приближение к тому, чего хочет человек.

Автор считает, что эта проблема остается широко открытой. Его интересует эта проблема, потому что он думает, что "обычный" прогресс в области ИИ, вероятно, приведет к способности относительно хорошо предсказывать поведение человека и имитировать работу экспертов. Откуда вытекает вопрос: что нам нужно знать для решения проблемы безопасности ИИ, помимо того, что нам нужно знать для создания ИИ?

## Узкие области

Мы можем решить очень простую проблему вывода цели в достаточно узких областях задач, где люди могут вести себя примерно рационально и простая модель ошибок достаточно точна.

Но в долгосрочной перспективе возникнут проблемы. Такой подход может сработать для такой тривиальной проблемы как перемещение из точки А в точку Б, но, вероятно, не сработает при проектировании города, управлении компанией или выработке правильной политики.

Возможно, существует подход, использующий обратное обучение с подкреплением в простых областях в качестве “кирпича” для решения всей проблемы управления ИИ.

Может существовать подход, который использует обратное обучение с подкреплением в простых областях в качестве строительного блока для решения всей проблемы управления ИИ. Возможно, это даже не такой уж и сложный подход. Но это не тривиальная проблема, и я не думаю, что от нее можно легко отказаться без каких-то новых идей.

## Моделирование “ошибок” фундаментально

Если мы хотим выполнить задачу не хуже эксперта, обратное обучение с подкреплением, безусловно, является мощным подходом.

Но в долгосрочной перспективе многие важные приложения требуют, чтобы ИИ принимал решения лучше, чем имеющиеся эксперты-люди. В этом контексте мы не можем использовать обычную парадигму — "более точные модели лучше"

Возможный дополнительный импульс обратному обучению с подкреплением дает явная модель ошибок или ограниченной рациональности человека. Именно она определяет, что ИИ должен делать по-другому, чтобы стать "умнее", какие части человеческих политик ему следует отбросить. Таким образом, она неявно определяет, какие из человеческих моделей поведения ИИ должен сохранить

## Моделировать "ошибки" сложно

В то же время автор считает, что написать модель человеческих недостатков, которая описывает, как люди отходят от рационального преследования фиксированных целей, будет не легче, чем написать полную модель человеческого поведения

## И что?

Вполне разумно придерживаться позиции "Ну, мы будем решать эту проблему, когда она возникнет". Но автор считает, что есть несколько вещей, которые стоит сделать заранее.

Исследования в области обратного обучения с подкреплением / вывода целей для дальнейшего управления ИИ, вероятно, должны уделять особое внимание проблеме ошибок проектирования и проблемам, возникающим при попытке найти политику, лучшую, чем та, на которой вы учитесь.

Стоит провести больше теоретических исследований, чтобы понять такого рода трудности и способы их решения. Эти исследования могут помочь определить другие практические подходы к управлению ИИ, которые затем можно будет изучить эмпирически.