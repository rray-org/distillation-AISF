# Learning from human preferences

Компания OpenAI представила алгоритм обучения, который эффективно использует минимальное количество обратной связи от человека для успешного выполнения сложных задач в средах обучения с подкреплением (RL). Системы машинного обучения с обратной связью от человека уже изучались и раньше, но этот подход был расширен, чтобы иметь возможность работать над гораздо более сложными задачами

Этот алгоритм продемонстрировали на примере обучения сальто назад. Несмотря на кажущуюся простоту задачи, определить конкретные инструкции для выполнения сальто назад непросто. Однако алгоритму это удалось и потребовалось всего 900 битов обратной связи от человека-оценщика

Процесс обучения проходит по трехступенчатому циклу:

- обучение с учителем (оценка человека)
- осознание агентом цели
- обучение RL

**Первый этап**

Изначально агент ИИ действует в среде случайным образом. Потом два видеоролика с его поведением предъявляются человеку, и он выбирает тот, который наиболее близок к достижению цели

**Второй этап**

Затем ИИ строит модель соответствующую цели задачи, определяя функцию вознаграждения по полученным оценкам человека

**Третий этап**

Затем используется RL, чтобы узнать, как достичь этой цели. По мере того, как поведение системы улучшается, она продолжает запрашивать мнение людей о парах траекторий, где она наиболее не уверена в том, какая из них лучше, и еще больше совершенствует свое понимание цели.

Сальто назад требует менее 1000 бит обратной связи от человека менее чем за час оценки. Способность алгоритма эффективно использовать эту обратную связь иллюстрируется его работой в различных симуляторах робототехники и играх Atari без знания о том, за что получаются очки вознаграждения. Агенты, обученные по этой методике, демонстрируют в этих средах сильные, а иногда и сверхчеловеческие способности.

Примечательно, что алгоритм не зависит от соответствия обратной связи типичной для среды функции вознаграждения — такой, которая нам как людям кажется более естественной. Например, в гоночной игре агенты могут быть обучены поддерживать скорость движения относительно других машин, что отличается от стандартного подхода, основанного на максимизации очков. Авторы показали, что во многих случаях обратная связь от человека может позволить нам определить конкретную цель более интуитивно и быстро, чем это возможно при ручной формулировке цели.