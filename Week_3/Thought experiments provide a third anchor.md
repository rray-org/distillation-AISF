# Thought experiments provide a third anchor

Я уже писал, что стоит ожидать от будущих ML систем новых способностей, явно в них не заложенных. Это довольно общее явление в физике - More Is Different.

Меня беспокоит это, так как мы привыкли предсказывать будущее смотря на уже существующий тренд. Однако, More Is Different требует ожидать нового, качественно иного поведения.

В таком случае, как предсказать на что будут похожи будущие ML системы? Я нашел полезным думать об этом в терминах “якорей” - опорных классов, в широком смысле аналогичных будущим ML системам.

Наиболее очевидный опорный класс - текущие ML системы. Я назвал его current ML классом. Я думаю, что это, как ни странно, хорошая стартовая точка, но она  не позволяет думать о возникающих способностях.

Какими могут быть другие якоря? Интуитивно, мы можем посмотреть на то, что люди умеют делать хорошо, а текущие ML системы нет. Это может быть:

- умелое использование внешних инструментов (калькуляторы, поисковики, прочие существующие программы и программирование в целом)
- очень эффективное обучение (к примеру, один раз прочесть книгу, чтобы выучить новый предмет)
- долговременное планирование (к примеру, успешно достигать целей на масштабе месяцев)

Через некоторое время, скорее всего, модели смогут делать подобное. Когда? Неизвестно. Это будем называть human anchor.

У этого якоря есть недостаток - высокий риск антропоморфизации.

Что приводит нас к третьему якорю - optimization anchor, что я связываю с подходом мысленных экспериментов, которые описывал ранее. Мы можем подумать, а что делал бы идеальный оптимизатор в данном сценарии. Такой подход породил идею Ника Бострома об скрепочном максимизаторе. Вот еще несколько примеров:

- optimization anchor верно предсказал imitative deception (Lin et al., 2021), так как система заточенная под высокоэффективное производство не имеет внутренних причин быть честной (правдивой)
- … стремление к власти это инструментальная цель полезная для множества различных прочих целей, поэтому является частью оптимальной политики

Идеи придуманные с помощью optimization anchor обычно сталкиваются со скептицизмом, так как противоречат current ML якорю, а также не имеют интуитивной привлекательности human anchor. Но эта разница и есть именно то, что делает этот якорь ценным. Если вы (как и я) чувствуете, что current ML и human anchors создают неполную картину, стоит воспользоваться третьей независимой перспективой.

Optimization anchor имеет ограничения. Так как он представляет ML систему как идеальный оптимизатор, он игнорирует большинство практических фактов о нейронных сетях. Это может привести к неверным прогнозам и к игнорированию свойств, которые по моему мнению будут необходимы для алаймента. Я расскажу об этом позже, но для вот некоторые важные свойства: нейронные сети обобщают информацию “естественным” образом, мы можем смотреть на внутренние репрезентации сети, динамика обучения плавная и без разрывов. Исследователи, придающие большой вес optimization anchor не полностью игнорируют эти факты, однако, я думаю, склонны недооценивать их и потому чрезмерно пессимистичны.

**Ценность мысленных экспериментов**

Якорь оптимизации указывает на ценность мысленных экспериментов в общем смысле. Он также предлагает мысленный эксперимент "Что если бы искусственный интеллект был идеальным оптимизатором?", однако существует множество других мысленных экспериментов, которые могут дать то понимание, какое было бы сложно получить от якорей машинного обучения или человеческих якорей. В этом смысле мысленные эксперименты являются не одним якорем, а сразу генератором якорей, что выглядит довольно ценным.

Один из мысленных экспериментов, который мне особенно нравится, это: что произойдет, если большая часть обучения агента происходит не во время градиентного спуска, а через обучение в контексте? Это, вероятно, произойдет в конце концов: агенты машинного обучения будут развертываться на более длительных временных горизонтах (подумайте об искусственных цифровых помощниках) и постепенно будет улучшаться качество обучения в контексте. Когда это произойдет, вероятно, поведение агентов будет контролироваться меньше "внешним" формированием градиентного спуска и больше какими-либо "внутренними" мотивами, которые у них случайно возникнут. Это также кажется изменением, которое может произойти внезапно, поскольку градиентный спуск медленный, а обучение в контексте быстрое.

Было бы замечательно, если бы у нас образовалось сообщество исследователей, проводящих мысленные эксперименты с чётко сформулированными предположениями, подробно объясняющими последствия этих предположений и, в идеале, связывающих это с современными исследованиями.

**Другие якоря**

Существует множество других якорей, которые могут быть полезны для прогнозирования будущих систем машинного обучения. Поведение животных может предоставить более широкий класс объектов, которые можно использовать как отправную точку (якорь), чем только человек. Эволюция и экономика являются примерами мощных, распределенных процессов оптимизации. Меня больше всего волнует лучшее понимание сложных систем, которые включают в себя биологические системы, мозги, организации, экономики и экосистемы и таким образом охватывают большинство обсуждаемых до сих пор классов якорей. 

Мне кажется, что сложные системы получили несправедливо мало внимания по сравнению с их релевантностью к машинному обучению. Действительно, эмерджентность сама по себе является концепцией из теории сложных систем, которая полезна для понимания недавних разработок в машинном обучении.

**Ограничения мысленных экспериментов**

До сих пор я сосредоточился на предсказании проблем, которые нам нужно решить. Но в какой-то момент нам действительно приходится с ними сталкиваться. В этом смысле мысленные эксперименты слабы, так как они часто указывают на важные проблемы широкого спектра, но, на мой взгляд, плохо справляются с детализацией, что необходимо для инженерного прогресса. 

Например, ранние мысленные эксперименты рассматривали единственную ИИ систему, которая была гораздо мощнее любых других современных технологий, хотя, скорее всего, на самом деле будет много систем машинного обучения с непрерывным распределением возможностей. 

Более новые мысленные эксперименты накладывают дискретные абстракции, такие как "цели" и "задачи", которые, как я думаю, не будут четко соотноситься с реальными системами машинного обучения. Таким образом, хотя мысленные эксперименты могут указывать на общие идеи для исследования, даже приведение этих идей к онтологии систем машинного обучения может быть сложной задачей.

В результате, хотя мы не можем слепо экстраполировать эмпирические тенденции, нам действительно нужны скоординированные усилия, основанные на эмпирике, чтобы решить будущие риски машинного обучения. Я объясню, почему считаю это возможным в следующем посте, но сначала проведу вас через пример "серьезного отношения к мысленному эксперименту" и что это подразумевает о возможных режимах отказа систем машинного обучения.