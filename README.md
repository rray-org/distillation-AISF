# distillation

Здесь собраны краткие версии статей из курса [AI Safety Fundamentals](https://course.aisafetyfundamentals.com/alignment) от BlueDot Impact. Предполагается, что вы сможете ознакомиться с технической составляющей AI Safety за неделю.

**Reward misspecification and instrumental convergence**
1. [Specification gaming: the flip side of AI ingenuity by Victoria Krakovna, Jonathan Uesato, Vladimir Mikulik et al. (2020)](https://github.com/rray-org/distillation-AISF/blob/main/Week_2/Specification%20gaming%20the%20flip%20side%20of%20AI%20ingenuity.md)
2. [Learning from Human Preferences by Paul Christiano, Alex Ray and Dario Amodei (2017)](https://github.com/rray-org/distillation-AISF/blob/main/Week_2/Learning%20from%20human%20preferences.md)
3. [Learning to Summarize with Human Feedback by Jeffrey Wu, Nisan Stiennon, Daniel Ziegler et al. (2020)](https://github.com/rray-org/distillation-AISF/blob/main/Week_2/Learning%20to%20Summarize%20with%20Human%20Feedback.md)
4. [The alignment problem from a deep learning perspective by Richard Ngo, Soeren Mindermann and Lawrence Chan (2022)](https://github.com/rray-org/distillation-AISF/blob/main/Week_2/The%20alignment%20problem%20from%20a%20deep%20learning%20perspective.md)
5. [Optimal Policies Tend To Seek Power by Alex Turner, Logan Smith, Rohin Shah et al. (2021)](https://github.com/rray-org/distillation-AISF/blob/main/Week_2/Optimal%20Policies%20Tend%20To%20Seek%20Power.md)
6. [What failure looks like by Paul Christiano (2019)](https://github.com/rray-org/distillation-AISF/blob/main/Week_2/What%20failure%20looks%20like.md)
7. [Inverse reinforcement learning example by Udacity (2016)](https://github.com/rray-org/distillation-AISF/blob/main/Week_2/Inverse%20reinforcement%20learning%20example.md)
9. [The easy goal inference problem is still hard by Paul Christiano (2018)](https://github.com/rray-org/distillation-AISF/blob/main/Week_2/The%20easy%20goal%20inference%20problem%20is%20still%20hard.md)

**Goal misgeneralisation**
1. [Goal Misgeneralisation: Why Correct Specifications Aren’t Enough For Correct Goals by Rohin Shah (2022)](https://github.com/rray-org/distillation-AISF/blob/main/Week_3/Goal%20Misgeneralisation%20Why%20Correct%20Specifications.md)
2. [Goal Misgeneralization: Why Correct Specifications Aren’t Enough For Correct Goals by Rohin Shah and Vikrant Varma (2022)](https://arxiv.org/abs/2210.01790)
3. [Thought experiments provide a third anchor by Jacob Steinhardt (2022)](https://github.com/rray-org/distillation-AISF/blob/main/Week_3/Thought%20experiments%20provide%20a%20third%20anchor.md)
4. [ML Systems Will Have Weird Failure Modes by Jacob Steinhardt (2022)](https://bounded-regret.ghost.io/ml-systems-will-have-weird-failure-modes-2/)
5. [The alignment problem from a deep learning perspective by Richard Ngo, Soeren Mindermann and Lawrence Chan (2022)](https://arxiv.org/abs/2209.00626)
6. [What failure looks like by Paul Christiano (2019)](https://github.com/rray-org/distillation-AISF/blob/main/Week_3/What%20failure%20looks%20like.md)

**Task decomposition for scalable oversight**
1. [AI Alignment Landscape by Paul Christiano (2020)](https://github.com/rray-org/distillation-AISF/blob/main/Week_4/AI%20Alignment%20Landscape.md)
2. [Measuring Progress on Scalable Oversight for Large Language Models by Samuel Bowman (2022)](https://github.com/rray-org/distillation-AISF/blob/main/Week_4/Measuring%20Progress%20on%20Scalable%20Oversight%20for%20Large.md)
3. [Learning Complex Goals with Iterated Amplification by Paul Christiano and Dario Amodei (2018)](https://github.com/rray-org/distillation-AISF/blob/main/Week_4/Learning%20Complex%20Goals%20with%20Iterated%20Amplification.md)
4. [Supervising strong learners by amplifying weak experts by Paul Christiano, Dario Amodei and Buck Shlegeris (2018)](https://arxiv.org/abs/1810.08575)
5. [Summarizing Books with Human Feedback by Jeffrey Wu, Ryan Lowe and Jan Leike (2021)](https://github.com/rray-org/distillation-AISF/blob/main/Week_4/Summarizing%20Books%20with%20Human%20Feedback.md)
6. [Language Models Perform Reasoning via Chain of Thought by Jason Wei, Denny Zhou and Google (2022)](https://github.com/rray-org/distillation-AISF/blob/main/Week_4/Language%20Models%20Perform%20Reasoning%20via%20Chain%20of%20Tho.md)
7. [Least-to-Most Prompting Enables Complex Reasoning in Large Language Models by Denny Zhou, Nathanael Scharli, Le Hou et al. (2022)](https://github.com/rray-org/distillation-AISF/blob/main/Week_4/Least-to-Most%20Prompting%20Enables%20Complex%20Reasoning.md

**Adversarial techniques for scalable oversight**
1. [AI-written critiques help humans notice flaws: blog post by Jan Leike, Jeffrey Wu, Catherine Yeh et al. (2022)](https://openai.com/blog/critiques/)
2. [AI safety via debate by Geoffrey Irving, Paul Christiano and Dario Amodei (2018)](https://arxiv.org/abs/1805.00899)
3. [Red-teaming language models with language models by Ethan Perez, Saffron Huang, Francis Song et al. (2022)](https://github.com/rray-org/distillation-AISF/blob/main/Week_5/Red-teaming%20language%20models%20with%20language%20models.md)
4. [Robust Feature-Level Adversaries are Interpretability Tools by Casper (2021)](https://arxiv.org/abs/2110.03605)

**Interpretability**
1. [Zoom In: An Introduction to Circuits by Chris Olah, Nick Cammarata, Ludwig Schubert et al. (2020)](https://distill.pub/2020/circuits/zoom-in/)
2. [Toy models of superposition by Nelson Elhage, Tristan Hume, Catherine Olsson et al. (2022)](https://transformer-circuits.pub/2022/toy_model/index.html)
3. [Understanding intermediate layers using linear classifier probes by Guillaume Alain and Yoshua Bengio (2016)](https://arxiv.org/abs/1610.01644)
4. [Discovering language model behaviors with model-written evaluations: blog post by Ethan Perez (2022)](https://www.alignmentforum.org/posts/yRAo2KEGWenKYZG9K/discovering-language-model-behaviors-with-model-written)
5. [Locating and Editing Factual Associations in GPT by Kevin Meng, David Bau, Alex Andonian et al. (2022)](https://rome.baulab.info/)

**Agent foundations**
1. [What is AIXI? by Marcus Hutter (2020)](https://github.com/rray-org/distillation-AISF/blob/main/Week_8/What%20is%20AIXI%20by%20Marcus%20Hutter%20(2020).md)
2. [Embedded Agents by Scott Garrabrant and Demski (2018)](https://intelligence.org/2018/10/29/embedded-agents/)
3. [Logical decision theory by Eliezer Yudkowsky (2017)](https://arbital.com/p/logical_dt/?l=5d6)
4. [Logical Induction: Blog post by Nate Soares (2016)](https://github.com/rray-org/distillation-AISF/blob/main/Week_8/Logical%20Induction.md)
5. [Progress on Causal Influence diagrams: blog post by Tom Everitt, Ryan Carey, Lewis Hammond et al. (2021)](https://deepmindsafetyresearch.medium.com/progress-on-causal-influence-diagrams-a7a32180b0d1)
6. [Avoiding Side Effects By Considering Future Tasks by Victoria Krakovna, Laurent Orseau, Richard Ngo et al. (2020)](https://arxiv.org/abs/2010.07877)
7. [Cooperation, Conflict and Transformative AI by Jesse Clifton (2019)](https://www.alignmentforum.org/s/p947tK8CoBbdpPtyK/p/KMocAf9jnAKc2jXri)
