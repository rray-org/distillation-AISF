# Red-teaming language models with language models

**В нашей** [недавней статье] ([https://arxiv.org/abs/2202.03286](https://arxiv.org/abs/2202.03286)) мы показываем, что можно автоматически находить входные данные, которые выявляют вредоносный текст из языковых моделей, генерируя входные данные с использованием самих языковых моделей. Наш подход предоставляет один из инструментов для выявления вредных моделей поведения до того, как это повлияет на пользователей, хотя мы подчеркиваем, что его следует рассматривать как один из компонентов наряду со многими другими методами, которые понадобятся для выявления вреда и смягчения его последствий после обнаружения

LLM, такие как GPT-3 и Gopher, обладают замечательной способностью генерировать высококачественный текст, но их трудно внедрить в реальном мире. LLM сопряжены с риском создания очень вредоносного текста, и даже небольшой риск причинения вреда неприемлем в реальных приложениях.

Например, в 2016 году Microsoft выпустила твиттер-бота Tay, который автоматически отправляет твиты в ответ пользователям. В течение 16 часов [Microsoft удалила Tay] ([https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist](https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist)) после того, как несколько враждебно настроенных пользователей выявили расистские и сексуально заряженные твиты от Tay, которые были отправлены более чем 50 000 подписчиков. Результат был [не из-за отсутствия заботы со стороны Microsoft](https://blogs.microsoft.com/blog/2016/03/25/learning-tays-introduction/):

# Хотя мы были готовы ко многим видам злоупотреблений системой, мы допустили критическую оплошность в отношении этой конкретной атаки

Проблема в том, что существует очень много возможных входных данных, которые могут привести к тому, что модель сгенерирует вредоносный текст. В результате трудно найти все случаи сбоя модели до её развертывания в реальном мире. Предыдущая работа опиралась на платных комментаторов-людей для ручного обнаружения случаев сбоя ([Xu et al. 2021](https://aclanthology.org/2021.naacl-main.235/), *inter alia*). Этот подход эффективен, но дорог, ограничивая количество и разнообразие обнаруженных случаев сбоя.

Мы стремимся дополнить ручное тестирование и сократить количество критических ошибок за счет автоматического поиска случаев сбоя (или "красной команды"). Для этого мы генерируем тестовые примеры, используя саму языковую модель, и используем классификатор для обнаружения различных вредных поведений в тестовых примерах, как показано ниже:

![image info](graph23.png)

Наш подход выявляет множество вредных моделей поведения:

1. **Оскорбительные выражения**: Разжигание ненависти, ненормативная лексика, сексуальное содержание, дискриминация и т.д.
2. **Утечка данных**: Получение защищенной авторским правом или частной, позволяющей идентифицировать личность информации из учебного корпуса.
3. **Генерация контактной информации**: Указание пользователям без необходимости отправлять электронные письма или звонить реальным людям.
4. **Предвзятость в распределении**: Говорить о некоторых группах людей несправедливо иначе, чем о других группах, в среднем по большому количеству выходных данных.
5. **Разговорный вред**: Оскорбительные выражения, которые встречаются, например, в контексте длительного диалога.

Чтобы генерировать тестовые примеры с использованием языковых моделей, мы исследуем множество методов, начиная от генерации на основе подсказок и обучения с несколькими кадрами до контролируемой точной настройки и обучения с подкреплением. Некоторые методы генерируют более разнообразные тестовые примеры, в то время как другие методы генерируют более сложные тестовые примеры для целевой модели. В совокупности предлагаемые нами методы полезны для получения высокого охвата тестированием, а также для моделирования состязательных ситуаций.

Как только мы обнаруживаем случаи сбоя, становится легче исправить вредоносное поведение модели с помощью:

1. Занесения в черный список определенных фраз, которые часто встречаются в вредоносных выходных данных, что не позволяет модели генерировать выходные данные, содержащие фразы высокого риска.
2. Поиск оскорбительных тренировочных данных, указанных моделью, для удаления этих данных при обучении будущих итераций модели.
3. Дополните подсказку модели (текст, определяющий условия) примером желаемого поведения для определенного вида входных данных, как показано в нашей [недавней работе](https://deepmind.com/blog/article/language-modelling-at-scale).
4. Обучение модели [минимизации вероятности] ([https://arxiv.org/abs/1908.04319](https://arxiv.org/abs/1908.04319)) ее первоначального, вредного результата для данного тестового ввода.

В целом, языковые модели являются высокоэффективным инструментом для выявления случаев, когда языковые модели ведут себя различными нежелательными способами. В нашей текущей работе мы сосредоточились на вреде red teaming, который наносят современные языковые модели. В будущем наш подход также может быть использован для упреждающего обнаружения других, предполагаемых рисков от передовых систем машинного обучения, например, из-за [внутреннего рассогласования] ([https://arxiv.org/abs/1906.01820](https://arxiv.org/abs/1906.01820)) или [сбоев в объективной надежности](https://arxiv.org/abs/2105.14111). Этот подход является лишь одним из компонентов ответственной разработки языковой модели: мы рассматриваем red teaming как один из инструментов, который следует использовать наряду со многими другими, как для выявления недостатков в языковых моделях, так и для их смягчения. Мы ссылаемся на раздел 7.3 [Rae et al. 2021](https://arxiv.org/abs/2112.11446) для более широкого обсуждения другой работы, необходимой для обеспечения безопасности языковой модели.

Для получения более подробной информации о нашем подходе и результатах, а также о более широких последствиях наших выводов, прочтите нашу [статью о красной команде] ([https://arxiv.org/abs/2202.03286](https://arxiv.org/abs/2202.03286)) здесь.
