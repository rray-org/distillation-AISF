# Summarizing Books with Human Feedback

Для безопасного применения мощного искусственного интеллекта общего назначения (AGI) в будущем нам необходимо обеспечить, чтобы модели машинного обучения действовали в соответствии с намерениями человека. Эта задача стала известна как *проблема  алаймента*.

Масштабируемое решение проблемы алаймента должно работать в задачах, где оценка результатов модели затруднена или занимает много времени. Чтобы протестировать масштабируемые методы выравнивания, мы обучили модель резюмировать целые книги, как показано в следующих примерах.

> Эти примеры были выбраны из произведений, находящихся в [общественном достоянии] ([https://www.gutenberg.org/policy/license.html](https://www.gutenberg.org/policy/license.html)), и являются частью данных предварительного обучения GPT-3. Для контроля этого эффекта и чисто в исследовательских целях в нашей [статье](https://arxiv.org/abs/2109.10862) оцениваются краткие изложения книг, которые модель никогда раньше не видела.
> 

Наша модель работает, сначала обобщая небольшие фрагменты книги, затем сводя их в резюме более высокого уровня, и так далее.

Наша лучшая модель, тонко настроенная на основе GPT-3, генерирует разумные резюме целых книг, иногда даже соответствуя среднему качеству написанных человеком резюме: она достигает оценки 6/7 (аналогичной средней оценке написанного человеком резюме) от людей, прочитавших книгу, в 5 % случаев и оценки 5/7 в 15 % случаев. Наша модель также достигла передовых результатов на наборе данных [BookSum dataset] ([https://arxiv.org/abs/2105.08209](https://arxiv.org/abs/2105.08209)) при резюмировании длинных книг. Модель, отвечающая на вопросы с нулевым результатом, может использовать резюме нашей модели для получения конкурентоспособных результатов на наборе данных [NarrativeQA](https://arxiv.org/abs/1712.07040) для ответа на вопросы в объеме книги.

> Мы изменили наше первоначальное утверждение о результатах на NarrativeQA после того, как нам стало известно о предыдущих работах с лучшими результатами, чем у нас.
> 

## **Наш подход: сочетание обучения с подкреплением на основе обратной связи с человеком (RLHF) и рекурсивной декомпозиции задачи**.

Рассмотрим задачу резюмирования фрагмента текста. Большие [предварительно обученные модели не очень хорошо справляются с резюмированием](https://openai.com/blog/learning-to-summarize-with-human-feedback/). В прошлом мы обнаружили, что обучение модели с помощью [reinforcement learning from human feedback](https://openai.com/blog/deep-reinforcement-learning-from-human-preferences/) помогает согласовать резюме модели с предпочтениями человека в отношении коротких постов и статей. Но для оценки аннотаций к целым книгам требуется много усилий, поскольку человеку нужно прочитать всю книгу, что занимает много часов.

Чтобы решить эту проблему, мы дополнительно используем *рекурсивную декомпозицию задачи*: мы процедурно разбиваем сложную задачу на более простые. В данном случае мы разбиваем резюмирование длинного куска текста на резюмирование нескольких более коротких кусков. По сравнению с процедурой сквозного обучения рекурсивная декомпозиция задач имеет следующие преимущества:

1. Декомпозиция позволяет людям быстрее оценивать краткие описания моделей, используя краткие описания небольших частей книги, вместо того чтобы читать исходный текст.
2. Легче проследить процесс составления резюме. Например, можно проследить, где в исходном тексте происходят определенные события из резюме. Убедитесь в этом сами на сайте [our summary explorer](https://openaipublic.blob.core.windows.net/recursive-book-summ/website/index.html)!
3. Наш метод можно использовать для краткого изложения книг неограниченного объема, не ограниченного длиной контекста используемых нами моделей трансформации.

## **Почему мы работаем над этим**

Эта работа является частью нашего [продолжающегося](https://openai.com/blog/amplifying-ai-training/) [исследования](https://openai.com/blog/debate/) по согласованию передовых систем ИИ, что является ключевым моментом [нашей миссии](https://openai.com/about/) По мере того как мы обучаем наши модели выполнять все более сложные задачи, человеку становится все труднее делать обоснованные оценки результатов работы моделей. Это усложняет обнаружение тонких проблем в результатах работы моделей, которые могут привести к негативным последствиям при их применении. Поэтому мы хотим, чтобы наши возможности по оценке моделей росли так же быстро как их возможности.

Наш текущий подход к решению этой проблемы заключается в [расширении возможностей людей по оценке результатов моделей машинного обучения с помощью других моделей](https://deepmindsafetyresearch.medium.com/scalable-agent-alignment-via-reward-modeling-bf4ab06dfd84). В данном случае для оценки аннотаций к книгам мы предоставляем людям аннотации к отдельным главам, написанным нашей моделью, что позволяет сэкономить время при оценке аннотаций по сравнению с чтением исходного текста. Наши успехи в резюмировании книг — это первая крупномасштабная эмпирическая работа по методам выравнивания масштаба.

В дальнейшем мы исследуем лучшие способы помощи людям в оценке поведения моделей, чтобы найти методы, которые можно масштабировать до уровня искусственного общего интеллекта (AGI).